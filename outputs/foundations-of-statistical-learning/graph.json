{
  "nodes": [
    {
      "id": "probability-and-statistics-basics",
      "label": "Probability and Statistics Basics",
      "summary": "Fundamentals of probability (random variables, distributions, expectation, etc.) underpin statistical learning and are needed to define loss expectations and risk\u3010^3\u3011\u3010^1\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        3,
        1
      ]
    },
    {
      "id": "linear-algebra-basics",
      "label": "Linear Algebra Basics",
      "summary": "Linear algebra skills (vectors, matrices, transformations) are essential for many models (e.g., linear regression, PCA) because inputs are represented in vector space\u3010^1\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        1
      ]
    },
    {
      "id": "calculus-and-optimization-basics",
      "label": "Calculus and Optimization Basics",
      "summary": "Calculus (derivatives, gradients) and basic optimization knowledge are required to train models (e.g. gradient descent for neural networks or SVMs) and to understand loss minimization\u3010^4\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        4
      ]
    },
    {
      "id": "statistical-learning-overview",
      "label": "Statistical Learning Overview",
      "summary": "Statistical learning theory frames machine learning as inferring a predictive function from data, integrating ideas from statistics (like hypothesis sets) and functional analysis\u3010^1\u3011\u3010^2\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        1,
        2
      ]
    },
    {
      "id": "supervised-vs-unsupervised-learning",
      "label": "Supervised vs. Unsupervised Learning",
      "summary": "Supervised learning uses labeled examples (X,Y pairs) to train predictors, whereas unsupervised learning finds structure in data without explicit labels\u3010^1\u3011\u3010^2\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        1,
        2
      ]
    },
    {
      "id": "regression-vs-classification",
      "label": "Regression vs. Classification",
      "summary": "In supervised learning, regression refers to predicting a continuous output, while classification refers to predicting a discrete category\u3010^1\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        1
      ]
    },
    {
      "id": "parametric-vs-nonparametric",
      "label": "Parametric vs. Nonparametric Methods",
      "summary": "Parametric methods assume a specific functional form for the model (with fixed parameters), whereas nonparametric methods make fewer assumptions and can adapt complexity to the data\u3010^2\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        2
      ]
    },
    {
      "id": "prediction-vs-inference",
      "label": "Prediction vs. Inference",
      "summary": "Statistical learning distinguishes between prediction (forecasting new data points accurately) and inference (understanding model structure or parameters)\u3010^2\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        2
      ]
    },
    {
      "id": "loss-function-and-risk",
      "label": "Loss Functions and Risk",
      "summary": "A loss function \\(\\ell(y,\\hat y)\\) quantifies prediction error, and the risk is the expected loss under the true data distribution. Models are selected to minimize risk (expected loss)\u3010^3\u3011\u3010^4\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        3,
        4
      ]
    },
    {
      "id": "empirical-risk-and-generalization",
      "label": "Empirical Risk and Generalization",
      "summary": "Empirical risk is the average loss on the training set, used to approximate true risk. The difference between empirical risk and true (generalization) risk can lead to overfitting error\u3010^4\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        4
      ]
    },
    {
      "id": "overfitting-and-underfitting",
      "label": "Overfitting and Underfitting",
      "summary": "Overfitting occurs when a model fits the training data too closely (including noise), leading to poor performance on new data. Underfitting occurs when a model is too simple to capture the underlying pattern in the data\u3010^1\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        1
      ]
    },
    {
      "id": "bias-variance-tradeoff",
      "label": "Bias-Variance Tradeoff",
      "summary": "Simpler models have high bias and low variance, whereas complex models have low bias and high variance. Balancing this tradeoff is crucial for generalization\u3010^6\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        6
      ]
    },
    {
      "id": "model-complexity-and-capacity",
      "label": "Model Complexity and Capacity",
      "summary": "The complexity or capacity of a model class (e.g. its VC dimension or number of parameters) affects generalization. More complex models can fit data better but risk overfitting\u3010^6\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        6
      ]
    },
    {
      "id": "regularization",
      "label": "Regularization and Penalties",
      "summary": "Regularization adds penalties or constraints to the learning objective (e.g. L2 norm) to control model complexity. This helps stabilize solutions and reduce overfitting\u3010^1\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        1
      ]
    },
    {
      "id": "cross-validation-and-selection",
      "label": "Cross-Validation and Model Selection",
      "summary": "Cross-validation and similar techniques estimate model performance on unseen data, guiding selection of model complexity or hyperparameters to avoid overfitting\u3010^4\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        4
      ]
    },
    {
      "id": "linear-regression",
      "label": "Linear Regression",
      "summary": "Linear regression models a continuous response as a linear combination of inputs. It is a parametric method whose parameters are typically estimated by minimizing squared error\u3010^1\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        1
      ]
    },
    {
      "id": "logistic-regression",
      "label": "Logistic Regression",
      "summary": "Logistic regression applies a logistic (sigmoid) transformation to a linear model to perform binary classification. Parameters are learned by maximizing likelihood (minimizing log-loss)\u3010^1\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        1
      ]
    },
    {
      "id": "decision-trees",
      "label": "Decision Trees",
      "summary": "Decision trees are flow-chart models that split data by feature tests to predict outputs. They can handle both classification (discrete output at leaves) and regression (continuous output) tasks\u3010^5\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        5
      ]
    },
    {
      "id": "k-nearest-neighbors",
      "label": "k-Nearest Neighbors (k-NN)",
      "summary": "KNN is a nonparametric method that predicts by finding the *k* closest training examples to a query and using majority voting (classification) or averaging (regression) of their outputs\u3010^9\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        9
      ]
    },
    {
      "id": "naive-bayes-classification",
      "label": "Naive Bayes Classification",
      "summary": "Naive Bayes classification applies Bayes\u2019 theorem with a strong independence assumption among features. It computes class probabilities for a given input based on feature likelihoods\u3010^8\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        8
      ]
    },
    {
      "id": "support-vector-machines",
      "label": "Support Vector Machines",
      "summary": "SVMs are supervised models that find a maximum-margin hyperplane to separate classes. They can handle nonlinearity via kernel functions by implicitly mapping data into high-dimensional feature spaces\u3010^10\u3011\u3010^10\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        10,
        10
      ]
    },
    {
      "id": "ensemble-methods",
      "label": "Ensemble Methods (Bagging, Boosting)",
      "summary": "Ensemble methods combine multiple base models (often decision trees) to improve performance. For example, bagged/boosted tree ensembles reduce variance of single trees and often yield higher accuracy\u3010^5\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        5
      ]
    },
    {
      "id": "neural-networks-basics",
      "label": "Neural Networks Basics",
      "summary": "Neural networks are layers of interconnected 'neurons' that can learn complex nonlinear functions. They are trained by optimizing a loss (e.g. via gradient descent) and are fundamental in deep learning\u3010^7\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        7
      ]
    },
    {
      "id": "principal-component-analysis",
      "label": "Principal Component Analysis (PCA)",
      "summary": "PCA is an unsupervised linear method that reduces dimensionality by projecting data onto orthogonal directions of maximal variance. It is useful for compressing data and mitigating high-dimensional issues\u3010^5\u3011.",
      "study_time_minutes": 30,
      "source_citations": [
        5
      ]
    },
    {
      "id": "curse-of-dimensionality",
      "label": "Curse of Dimensionality",
      "summary": "As dimensionality increases, data become sparse and distance metrics become less reliable. This phenomenon makes learning harder and motivates dimensionality reduction and regularization approaches.",
      "study_time_minutes": 30,
      "source_citations": []
    }
  ],
  "edges": [
    {
      "source": "probability-and-statistics-basics",
      "target": "loss-function-and-risk",
      "confidence": 0.9,
      "rationale": "Expected loss (risk) is defined using probability theory (expectations)",
      "source_citations": [
        3
      ]
    },
    {
      "source": "statistical-learning-overview",
      "target": "supervised-vs-unsupervised-learning",
      "confidence": 0.9,
      "rationale": "Knowing the difference between supervised and unsupervised tasks is fundamental to statistical learning theory",
      "source_citations": [
        1,
        2
      ]
    },
    {
      "source": "statistical-learning-overview",
      "target": "regression-vs-classification",
      "confidence": 0.9,
      "rationale": "Understanding regression vs. classification is required to choose appropriate models",
      "source_citations": [
        1
      ]
    },
    {
      "source": "statistical-learning-overview",
      "target": "parametric-vs-nonparametric",
      "confidence": 0.9,
      "rationale": "Distinguishing parametric and nonparametric methods guides the choice of model complexity",
      "source_citations": [
        2
      ]
    },
    {
      "source": "statistical-learning-overview",
      "target": "prediction-vs-inference",
      "confidence": 0.9,
      "rationale": "Understanding whether the goal is prediction or inference frames the learning approach",
      "source_citations": [
        2
      ]
    },
    {
      "source": "loss-function-and-risk",
      "target": "empirical-risk-and-generalization",
      "confidence": 0.9,
      "rationale": "Empirical risk (training error) vs true risk gap causes generalization issues",
      "source_citations": [
        4
      ]
    },
    {
      "source": "empirical-risk-and-generalization",
      "target": "overfitting-and-underfitting",
      "confidence": 0.9,
      "rationale": "Overfitting arises from fitting training data too closely, due to sampling effects",
      "source_citations": [
        4
      ]
    },
    {
      "source": "overfitting-and-underfitting",
      "target": "bias-variance-tradeoff",
      "confidence": 0.8,
      "rationale": "Overfitting and underfitting represent extremes of the bias-variance tradeoff",
      "source_citations": [
        6
      ]
    },
    {
      "source": "model-complexity-and-capacity",
      "target": "bias-variance-tradeoff",
      "confidence": 0.9,
      "rationale": "More complex models have lower bias but higher variance, affecting generalization",
      "source_citations": [
        6
      ]
    },
    {
      "source": "overfitting-and-underfitting",
      "target": "regularization",
      "confidence": 0.8,
      "rationale": "Regularization (penalties) is used to control complexity and reduce overfitting",
      "source_citations": [
        1
      ]
    },
    {
      "source": "bias-variance-tradeoff",
      "target": "regularization",
      "confidence": 0.8,
      "rationale": "Regularization provides a way to reduce variance (and increase bias) for better generalization",
      "source_citations": [
        1
      ]
    },
    {
      "source": "overfitting-and-underfitting",
      "target": "cross-validation-and-selection",
      "confidence": 0.9,
      "rationale": "Cross-validation estimates out-of-sample error to guide model selection and avoid overfitting",
      "source_citations": [
        4
      ]
    },
    {
      "source": "decision-trees",
      "target": "ensemble-methods",
      "confidence": 0.9,
      "rationale": "Ensemble methods like bagging and boosting are based on combining decision trees",
      "source_citations": [
        5
      ]
    },
    {
      "source": "probability-and-statistics-basics",
      "target": "naive-bayes-classification",
      "confidence": 0.9,
      "rationale": "Naive Bayes classification relies on probability theory (Bayes\u2019 theorem) as a foundation",
      "source_citations": [
        8
      ]
    },
    {
      "source": "supervised-vs-unsupervised-learning",
      "target": "k-nearest-neighbors",
      "confidence": 0.9,
      "rationale": "K-NN is a prototypical supervised learning algorithm requiring label data",
      "source_citations": [
        9
      ]
    },
    {
      "source": "calculus-and-optimization-basics",
      "target": "neural-networks-basics",
      "confidence": 0.7,
      "rationale": "Neural networks require calculus (gradients) for training via optimization",
      "source_citations": [
        7
      ]
    },
    {
      "source": "calculus-and-optimization-basics",
      "target": "support-vector-machines",
      "confidence": 0.8,
      "rationale": "SVM training is a convex optimization problem depending on calculus",
      "source_citations": [
        10
      ]
    },
    {
      "source": "linear-algebra-basics",
      "target": "principal-component-analysis",
      "confidence": 0.9,
      "rationale": "PCA uses linear algebra (matrix operations/eigenvectors) to reduce dimensionality",
      "source_citations": [
        5
      ]
    },
    {
      "source": "supervised-vs-unsupervised-learning",
      "target": "linear-regression",
      "confidence": 0.8,
      "rationale": "Linear regression is a classic supervised learning method",
      "source_citations": [
        1
      ]
    },
    {
      "source": "supervised-vs-unsupervised-learning",
      "target": "logistic-regression",
      "confidence": 0.8,
      "rationale": "Logistic regression is a supervised classification technique",
      "source_citations": [
        1
      ]
    },
    {
      "source": "supervised-vs-unsupervised-learning",
      "target": "decision-trees",
      "confidence": 0.8,
      "rationale": "Decision trees are a supervised learning model",
      "source_citations": [
        1
      ]
    },
    {
      "source": "supervised-vs-unsupervised-learning",
      "target": "naive-bayes-classification",
      "confidence": 0.8,
      "rationale": "Naive Bayes is a supervised classification method",
      "source_citations": [
        1
      ]
    },
    {
      "source": "supervised-vs-unsupervised-learning",
      "target": "support-vector-machines",
      "confidence": 0.8,
      "rationale": "SVMs are a supervised learning method",
      "source_citations": [
        1
      ]
    }
  ]
}